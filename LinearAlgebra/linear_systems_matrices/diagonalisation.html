
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Matrix Diagonalisation &#8212; Mathematics for Natural Sciences A</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Mathematics for Natural Sciences A</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Welcome to Mathematics for Natural Sciences A
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Prelim/notationfunctions.html">
   1. Mathematical Prelimaries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../complex_numbers/complex_numbers.html">
   2. Complex Numbers
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/LinearAlgebra/linear_systems_matrices/diagonalisation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FLinearAlgebra/linear_systems_matrices/diagonalisation.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#diagonalisation-a-x-lambda-x-1">
   Diagonalisation
   <span class="math notranslate nohighlight">
    \(A = X\Lambda X^{-1}\)
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#about-diagonalisation">
   About Diagonalisation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-independence">
   Linear Independence
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-eigenvalues">
   More Eigenvalues
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#algebraic-and-geometric-multiplicity">
   Algebraic and Geometric Multiplicity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#matrix-powers-a-k">
   Matrix Powers
   <span class="math notranslate nohighlight">
    \(A^k\)
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#complex-eigenvalues">
   Complex Eigenvalues
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rotations-in-2d">
     Rotations in 2D
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#trace-and-determinant">
   Trace and Determinant
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solutions-to-exercises">
   Solutions to Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Matrix Diagonalisation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#diagonalisation-a-x-lambda-x-1">
   Diagonalisation
   <span class="math notranslate nohighlight">
    \(A = X\Lambda X^{-1}\)
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#about-diagonalisation">
   About Diagonalisation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-independence">
   Linear Independence
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-eigenvalues">
   More Eigenvalues
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#algebraic-and-geometric-multiplicity">
   Algebraic and Geometric Multiplicity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#matrix-powers-a-k">
   Matrix Powers
   <span class="math notranslate nohighlight">
    \(A^k\)
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#complex-eigenvalues">
   Complex Eigenvalues
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rotations-in-2d">
     Rotations in 2D
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#trace-and-determinant">
   Trace and Determinant
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solutions-to-exercises">
   Solutions to Exercises
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="section" id="matrix-diagonalisation">
<h1>Matrix Diagonalisation<a class="headerlink" href="#matrix-diagonalisation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h2>
<p>Recall that a diagonal matrix is a square matrix with zeros everywhere except the main diagonal. Multiplying a vector by a diagonal matrix <span class="math notranslate nohighlight">\(D\)</span> is easy. Just multiply each entry of the vector by the the element in the corresponding position of the diagonal matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}d_{1}&amp;0&amp;\cdots&amp;0\\0&amp;d_{2}&amp;\cdots&amp;0\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0&amp;0&amp;\cdots&amp;d_{n}\end{pmatrix}\begin{pmatrix}x_1\\x_2\\\vdots\\x_n\end{pmatrix} = \begin{pmatrix}d_{1}x_1\\d_{2}x_2\\\vdots\\d_{n}x_n\end{pmatrix}.\end{split}\]</div>
<p>In fact, for diagonal matrices we immediately have the eigenvalues and eigenvectors! The eigenvalues are the diagonal entries <span class="math notranslate nohighlight">\(d_i\)</span> and the eigenvectors are the standard coordinate vectors <span class="math notranslate nohighlight">\(e_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[De_i = d_ie_i.\]</div>
<p>Multiplying by a diagonal matrix is easy because any vector is a linear sum of coordinate vectors. The diagonal entries of <span class="math notranslate nohighlight">\(D\)</span> act separately on each of the components of the vector <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
Dx &amp;= D(x_1e_1 + \cdots + Dx_ne_n)\\
&amp;= x_1De_1 + \cdots + x_nDe_n\\
&amp;= (x_1d_1)e_1 + \cdots + (x_nd_n)e_n
\end{align*}\end{split}\]</div>
<p>What about general (non-diagonal) square matrices? If <span class="math notranslate nohighlight">\(x\)</span> is an eigenvector of a square matrix <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span> its eigenvalue, then we have</p>
<div class="math notranslate nohighlight">
\[Ax = \lambda x.\]</div>
<p>If <span class="math notranslate nohighlight">\(x\)</span> is an eigenvector, <span class="math notranslate nohighlight">\(A\)</span> behaves like a diagonal matrix. Instead of writing <span class="math notranslate nohighlight">\(x\)</span> as a sum of coordinate vectors <span class="math notranslate nohighlight">\(e_i\)</span>, we write it as a sum of eigenvectors <span class="math notranslate nohighlight">\(v_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[ x = a_1v_1 + \cdots + a_nv_n,\]</div>
<p>Then</p>
<div class="math notranslate nohighlight">
\[ Ax = a_1\lambda_1v_1 + \cdots + a_n\lambda_nv_n.\]</div>
<p>We just need to find the values <span class="math notranslate nohighlight">\(a_i\)</span>. It turns out that these are just the inverse of the matrix of eigenvectors, as we will see in the next section.</p>
</div>
<div class="section" id="diagonalisation-a-x-lambda-x-1">
<h2>Diagonalisation <span class="math notranslate nohighlight">\(A = X\Lambda X^{-1}\)</span><a class="headerlink" href="#diagonalisation-a-x-lambda-x-1" title="Permalink to this headline">¶</a></h2>
<div class="admonition-matrix-diagonalisation admonition">
<p class="admonition-title">Matrix Diagonalisation</p>
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be an <span class="math notranslate nohighlight">\((n \times n)\)</span> matrix with <span class="math notranslate nohighlight">\(n\)</span> eigenvectors <span class="math notranslate nohighlight">\(v_i\)</span> and corresponding eigenvalues <span class="math notranslate nohighlight">\(\lambda_i\)</span>. Let</p>
<div class="math notranslate nohighlight">
\[\begin{split}X = \begin{pmatrix}|&amp;&amp;|\\v_1&amp;\cdots&amp;v_n\\|&amp;&amp;|\end{pmatrix}\end{split}\]</div>
<p>be the matrix of eigenvectors and</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Lambda = \begin{pmatrix}\lambda_1 &amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;\lambda_n\end{pmatrix} \end{split}\]</div>
<p>the diagonal matrix of eigenvalues. If <span class="math notranslate nohighlight">\(X\)</span> is invertible, then</p>
<div class="math notranslate nohighlight">
\[X^{-1}AX = \Lambda\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[A = X\Lambda X^{-1}.\]</div>
</div>
<p>Note that we use capital lambda <span class="math notranslate nohighlight">\(\Lambda\)</span> to represent the matrix of eigenvalues.</p>
<p>To see why the above result is true, note <span class="math notranslate nohighlight">\(X^{-1}AX=\Lambda\)</span> and <span class="math notranslate nohighlight">\(A = X\Lambda X^{-1}\)</span> are both exactly equivalent to:</p>
<div class="math notranslate nohighlight">
\[AX = X\Lambda.\]</div>
<p>Then the left hand side <span class="math notranslate nohighlight">\(AX\)</span> is <span class="math notranslate nohighlight">\(A\)</span> times the eigenvectors:</p>
<div class="math notranslate nohighlight">
\[\begin{split}AX = A\begin{pmatrix}|&amp;&amp;|\\v_1&amp;\cdots&amp;v_n\\|&amp;&amp;|\end{pmatrix} = \begin{pmatrix}|&amp;&amp;|\\\lambda_1v_1&amp;\cdots&amp;\lambda_nv_n\\|&amp;&amp;|\end{pmatrix}\end{split}\]</div>
<p>because <span class="math notranslate nohighlight">\(Av_i= \lambda_iv_i\)</span>.</p>
<p>Whereas right hand side <span class="math notranslate nohighlight">\(X\Lambda\)</span> is the eigenvectors times the eigenvalues:</p>
<div class="math notranslate nohighlight">
\[\begin{split}X\Lambda = \begin{pmatrix}|&amp;&amp;|\\v_1&amp;\cdots&amp;v_n\\|&amp;&amp;|\end{pmatrix}\begin{pmatrix}\lambda_1 &amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;\lambda_n\end{pmatrix} = \begin{pmatrix}|&amp;&amp;|\\\lambda_1v_1&amp;\cdots&amp;\lambda_nx_n\\|&amp;&amp;|\end{pmatrix}.\end{split}\]</div>
<p>Thus we see that <span class="math notranslate nohighlight">\(AX=X\Lambda\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<p>Diagonalise the <span class="math notranslate nohighlight">\((2 \times 2)\)</span> matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = \begin{pmatrix}0 &amp;-1 \\ -1 &amp;  0\end{pmatrix}.\end{split}\]</div>
<p><strong>Solution</strong></p>
<p>This matrix represents a reflection in the line <span class="math notranslate nohighlight">\(y=-x\)</span> so we can immediately write down two eigenvalues and corresponding eigenvectors:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\lambda_1 = 1, \quad v_1 = \begin{pmatrix}1\\-1\end{pmatrix},\\
\lambda_2 = -1, \quad v_2 = \begin{pmatrix}1\\1\end{pmatrix}.\end{split}\]</div>
<p>Therefore we can write the matrix of eigenvectors <span class="math notranslate nohighlight">\(X\)</span> and matrix of eigenvalues <span class="math notranslate nohighlight">\(\Lambda\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}X &amp;= \begin{pmatrix}1 &amp; 1\\-1&amp;1\end{pmatrix},\\
\Lambda &amp;= \begin{pmatrix}1&amp;0\\0&amp;-1\end{pmatrix}.\end{align*}\end{split}\]</div>
<p>To complete the diagonalisation we need to calculate <span class="math notranslate nohighlight">\(X^{-1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}X^{-1} = \frac{1}{\det (X)  } \mathbb{adj}(X) = \frac{1}{2}\begin{pmatrix}1&amp;-1\\1&amp;1\end{pmatrix} = \begin{pmatrix}\frac{1}{2}&amp;-\frac{1}{2}\\\frac{1}{2}&amp;\frac{1}{2}\end{pmatrix}.\end{split}\]</div>
<p>Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = X\Lambda X^{-1} = \begin{pmatrix}1 &amp; 1\\-1&amp;1\end{pmatrix}\begin{pmatrix}1&amp;0\\0&amp;-1\end{pmatrix}\begin{pmatrix}\frac{1}{2}&amp;-\frac{1}{2}\\\frac{1}{2}&amp;\frac{1}{2}\end{pmatrix}.\end{split}\]</div>
</div>
<p>The matrix of eigenvectors <span class="math notranslate nohighlight">\(X^{-1}\)</span> is also called the <em>change of basis</em> matrix. Multiplying a vector <span class="math notranslate nohighlight">\(x\)</span> by <span class="math notranslate nohighlight">\(X^{-1}\)</span> transforms it from standard coordinates <span class="math notranslate nohighlight">\(e_1, \ldots, e_n\)</span>  to eigenvector coordinates <span class="math notranslate nohighlight">\(v_1, \ldots, v_n\)</span>.</p>
<p>Suppose <span class="math notranslate nohighlight">\(x\)</span> is written as a sum of eigenvectors with coefficients <span class="math notranslate nohighlight">\(a_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}x = a_1v_1 + \cdots + a_nv_n = X\begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\end{split}\]</div>
<p>then left-multiplying by <span class="math notranslate nohighlight">\(X^{-1}\)</span> gives the vector of coefficients <span class="math notranslate nohighlight">\(a_i\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-two-by-two-diag">
<span class="eqno">()<a class="headerlink" href="#equation-eq-two-by-two-diag" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}= X^{-1}x.\end{split}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Diagonalisation is not unique</strong></p>
<p>1. If we write the eigenvalues and eigenvectors in a different order, we get a different matrix <span class="math notranslate nohighlight">\(X\)</span>. Swapping <span class="math notranslate nohighlight">\(v_1\)</span> and <span class="math notranslate nohighlight">\(v_2\)</span> we get another way to write <a class="reference internal" href="#equation-eq-two-by-two-diag">()</a>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = \begin{pmatrix}1 &amp; 1\\1&amp;-1\end{pmatrix}\begin{pmatrix}-1&amp;0\\0&amp;1\end{pmatrix}\begin{pmatrix}\frac{1}{2}&amp;\frac{1}{2}\\\frac{1}{2}&amp;-\frac{1}{2}\end{pmatrix}.\end{split}\]</div>
<p>However it is important that the order of the eigenvalues is the same as the order of the eigenvectors. If you swap the eigenvectors, you must remember to also swap the eigenvalues!</p>
<p>2. Any eigenvector can be multiplied by a constant. For example replacing <span class="math notranslate nohighlight">\(v_1\)</span> by <span class="math notranslate nohighlight">\(2v_1\)</span> in <a class="reference internal" href="#equation-eq-two-by-two-diag">()</a>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = \begin{pmatrix}2 &amp; 1\\-2&amp;1\end{pmatrix}\begin{pmatrix}1&amp;0\\0&amp;-1\end{pmatrix}\begin{pmatrix}\frac{1}{4}&amp;-\frac{1}{4}\\\frac{1}{2}&amp;\frac{1}{2}\end{pmatrix}.\end{split}\]</div>
<p>3. The eigenvalues <em>are</em> unique, although different diagonalisations may result in a different order.</p>
</div>
</div>
<div class="section" id="about-diagonalisation">
<h2>About Diagonalisation<a class="headerlink" href="#about-diagonalisation" title="Permalink to this headline">¶</a></h2>
<p>Not all matrices can be diagonalised. To diagonalise a matrix, the matrix of eigenvectors <span class="math notranslate nohighlight">\(X\)</span> must be invertible. In this section we will investigate conditions under which this is the case.</p>
<p>Recall that if <span class="math notranslate nohighlight">\(v\)</span> is an eigenvector then so is any multiple <span class="math notranslate nohighlight">\(av\)</span>. Suppose a <span class="math notranslate nohighlight">\((2 \times 2)\)</span> matrix has eigenvector <span class="math notranslate nohighlight">\(v\)</span>. Then the vector <span class="math notranslate nohighlight">\(2v\)</span> is also an eigenvector, but the eigenvector matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}X = \begin{pmatrix}|&amp;|\\v&amp;2v\\|&amp;|\end{pmatrix}\end{split}\]</div>
<p>is not invertible since the second column is a multiple of the first (and therefore its determinant is zero).</p>
<p>We can extend this idea to <span class="math notranslate nohighlight">\((3 \times 3)\)</span> matrices. Suppose we have two independent eigenvectors <span class="math notranslate nohighlight">\(v_2\)</span> and <span class="math notranslate nohighlight">\(v_2\)</span> and a third eigenvector <span class="math notranslate nohighlight">\(v_3\)</span> such that <span class="math notranslate nohighlight">\(v_3 = av_1 + bv_2\)</span> for some scalars <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. Then the matrix of eigenvectors</p>
<div class="math notranslate nohighlight">
\[\begin{split}X = \begin{pmatrix}|&amp;|&amp;|\\v_1&amp;v_2&amp;v_3\\|&amp;|&amp;|\end{pmatrix}\end{split}\]</div>
<p>is not invertible. To extend this idea to the general case, we need to introduce the important concept of <em>linear independence</em>.</p>
</div>
<div class="section" id="linear-independence">
<h2>Linear Independence<a class="headerlink" href="#linear-independence" title="Permalink to this headline">¶</a></h2>
<p>A set of vectors is linearly dependent if one vector can be written as a linear sum of the other vectors.</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>Let <span class="math notranslate nohighlight">\(v_1, \ldots, v_n \in \mathbb{R}^m\)</span> be a set of vectors.</p>
<p>The vectors are <strong>linearly independent</strong> if the equation</p>
<div class="math notranslate nohighlight">
\[a_1v_1 + \cdots + a_nv_n = 0\]</div>
<p>has only the trivial solution</p>
<div class="math notranslate nohighlight">
\[a_1 = \cdots = a_n = 0.\]</div>
<p>Otherwise, we say the vectors are <strong>linearly dependent</strong>.</p>
</div>
<p>If we have <span class="math notranslate nohighlight">\(n\)</span> linearly dependent vectors <span class="math notranslate nohighlight">\(v_i \in \mathbb{R}^n\)</span> then it is easy to show that the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}X = \begin{pmatrix}|&amp;&amp;|\\v_1&amp;\cdots &amp;v_n\\|&amp;&amp;|\end{pmatrix}\end{split}\]</div>
<p>is not invertible. Therefore we can extend the invertible matrix theorem with two more equivalent conditions for invertibility:</p>
<div class="admonition-invertible-matrix-theorem-ii admonition">
<p class="admonition-title">Invertible Matrix Theorem (II)</p>
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be an <span class="math notranslate nohighlight">\((n \times n)\)</span> matrix. Then the following statements are equivalent:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(A\)</span> is invertible.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{det}(A) \neq 0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(A\)</span> has <span class="math notranslate nohighlight">\(n\)</span> pivots.</p></li>
<li><p>The null space of <span class="math notranslate nohighlight">\(A\)</span> is 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(Ax=b\)</span> has a unique solution for every <span class="math notranslate nohighlight">\(b \in \mathbb{R}^n\)</span>.</p></li>
<li><p><strong>The columns of <span class="math notranslate nohighlight">\(A\)</span> are linearly independent.</strong></p></li>
<li><p><strong>The rows of <span class="math notranslate nohighlight">\(A\)</span> are linearly independent.</strong></p></li>
</ol>
</div>
</div>
<div class="section" id="more-eigenvalues">
<h2>More Eigenvalues<a class="headerlink" href="#more-eigenvalues" title="Permalink to this headline">¶</a></h2>
<div class="admonition-theorem admonition">
<p class="admonition-title">Theorem</p>
<p>Eigenvectors <span class="math notranslate nohighlight">\(v_1, \ldots, v_n\)</span> corresponding to distinct eigenvalues are linearly independent.</p>
<p>An <span class="math notranslate nohighlight">\((n \times n)\)</span> matrix with <span class="math notranslate nohighlight">\(n\)</span> distinct eigenvalues is diagonalisable.</p>
</div>
<p>To prove this, suppose that <span class="math notranslate nohighlight">\(v_1\)</span> and <span class="math notranslate nohighlight">\(v_2\)</span> are linearly dependent eigenvectors of the matrix <span class="math notranslate nohighlight">\(A\)</span> with distinct eigenvalues <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-indep">
<span class="eqno">()<a class="headerlink" href="#equation-eq-indep" title="Permalink to this equation">¶</a></span>\[v_2 = av_1\]</div>
<p>for some <span class="math notranslate nohighlight">\(a\neq 0\)</span>.</p>
<p>Multiply by A:</p>
<div class="math notranslate nohighlight">
\[\lambda_2v_2 = a\lambda_1v_1\]</div>
<p>then divide by <span class="math notranslate nohighlight">\(\lambda_2\)</span> (since they are distinct we can assume that at least one of the eigenvalues is nonzero).</p>
<div class="math notranslate nohighlight" id="equation-eq-indep-2">
<span class="eqno">()<a class="headerlink" href="#equation-eq-indep-2" title="Permalink to this equation">¶</a></span>\[v_2 = \frac{a\lambda_1}{\lambda_2}v_2.\]</div>
<p>Comparing <a class="reference internal" href="#equation-eq-indep">()</a> and <a class="reference internal" href="#equation-eq-indep-2">()</a> we see that <span class="math notranslate nohighlight">\(\lambda_1/\lambda_2=1\)</span> and so</p>
<p><span class="math notranslate nohighlight">\(\lambda_1 = \lambda_2\)</span>.</p>
<p>We have shown that two linearly dependent eigenvectors must have identical eigenvalues. We will not show it here, but it is not difficult to extend this to the general case: eigenvectors from distinct eigenvalues are linearly independent.</p>
<p>We can conclude that if an <span class="math notranslate nohighlight">\((n \times n)\)</span> matrix has <span class="math notranslate nohighlight">\(n\)</span> distinct eigenvalues then it has <span class="math notranslate nohighlight">\(n\)</span> linearly independent eigenvectors. By the invertible matrix theorem  we can therefore can conclude that its matrix of eigenvectors <span class="math notranslate nohighlight">\(X\)</span> is invertible, and the matrix is invertible.</p>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<p>Determine the characteristic equation of each of the following matrices and identify which are diagonalisable:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = \begin{pmatrix}0&amp;-1\\-1&amp;0\end{pmatrix}, \quad B=\begin{pmatrix}1&amp;1\\1&amp;1\end{pmatrix}, \quad  C=\begin{pmatrix}1&amp;1\\0&amp;1\end{pmatrix}.\end{split}\]</div>
<p><strong>Solution</strong></p>
<p><span class="math notranslate nohighlight">\(A\)</span> has characteristic polynomial <span class="math notranslate nohighlight">\(\lambda^2 - 1 = (\lambda-1)(\lambda+1)\)</span>. It has two distinct eigenvalues <span class="math notranslate nohighlight">\(\lambda_1=1\)</span>,<span class="math notranslate nohighlight">\(\lambda_2=-1\)</span> and therefore two independent eigenvectors. It is diagonalisable and we already calculated <span class="math notranslate nohighlight">\(A = X\Lambda X^{-1}\)</span> <a class="reference internal" href="#equation-eq-two-by-two-diag">()</a>.</p>
<p><span class="math notranslate nohighlight">\(B\)</span> has characteristic polynomial <span class="math notranslate nohighlight">\(\lambda^2-2\lambda = \lambda(\lambda - 2)\)</span>. It has two distinct eigenvalues <span class="math notranslate nohighlight">\(\lambda_1=0\)</span>,<span class="math notranslate nohighlight">\(\lambda_2=2\)</span> and therefore has two independent eigenvectors and is diagonalisable.</p>
<p><span class="math notranslate nohighlight">\(C\)</span> has characteristic polynomial <span class="math notranslate nohighlight">\(\lambda^2-2\lambda+1 = (\lambda-1)^2\)</span>. It has a single eigenvalue <span class="math notranslate nohighlight">\(\lambda_1=1\)</span>. To determine if <span class="math notranslate nohighlight">\(C\)</span> is diagonalisable, we need to check if there are two independent eigenvectors in the eigenspace of <span class="math notranslate nohighlight">\(\lambda_1\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathrm{Null}(C - \lambda_1I) = \mathrm{Null}\begin{pmatrix}0&amp;1\\0&amp;0\end{pmatrix} = \begin{pmatrix}1\\0\end{pmatrix}.\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(C\)</span> has only one linearly independent eigenvector and therefore it is not diagonalisable.</p>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Diagonalisability is <em>not</em> related to invertibility. Non-invertible matrices can be diagonalisable, as in <span class="math notranslate nohighlight">\(B\)</span> above. Likewise, non-diagonalisable matrices can be invertable as in <span class="math notranslate nohighlight">\(C\)</span> above.</p>
</div>
</div>
<div class="section" id="algebraic-and-geometric-multiplicity">
<h2>Algebraic and Geometric Multiplicity<a class="headerlink" href="#algebraic-and-geometric-multiplicity" title="Permalink to this headline">¶</a></h2>
<p>The eigenvalues of a <span class="math notranslate nohighlight">\((n \times n)\)</span> square matrix are the roots of its characteristic polynomial <span class="math notranslate nohighlight">\(f(\lambda)\)</span>. We have already seen that if there are <span class="math notranslate nohighlight">\(n\)</span> distinct roots then there are <span class="math notranslate nohighlight">\(n\)</span> linearly independent eigenvectors and the matrix is diagonalisable. For example a <span class="math notranslate nohighlight">\((3 \times 3)\)</span> matrix with the following characteristic polynomial is diagonalisable.</p>
<div class="math notranslate nohighlight">
\[f(\lambda) = (\lambda-1)(\lambda-2)(\lambda-3)\]</div>
<p>If there are fewer then <span class="math notranslate nohighlight">\(n\)</span> distinct roots then at least one of the roots must be <em>repeated</em>. For example, the characteristic polynomial</p>
<div class="math notranslate nohighlight" id="equation-eq-repeated-root">
<span class="eqno">()<a class="headerlink" href="#equation-eq-repeated-root" title="Permalink to this equation">¶</a></span>\[f(\lambda) =(\lambda-2)(\lambda-1)^2\]</div>
<p>results in two distinct eigenvalues <span class="math notranslate nohighlight">\(\lambda_1=2\)</span> and <span class="math notranslate nohighlight">\(\lambda_2=1\)</span>. <span class="math notranslate nohighlight">\(\lambda_2\)</span> is a repeated root with multiplicity <span class="math notranslate nohighlight">\(2\)</span> since the factor <span class="math notranslate nohighlight">\((\lambda-1)\)</span> divides the polynomial twice. To determine whether the matrix is diagonalisable, we need to determine how many independent eigenvectors there are in the eigenspace of each of the eigenvectors.</p>
<div class="admonition-theorem admonition">
<p class="admonition-title">Theorem</p>
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be a square matrix and <span class="math notranslate nohighlight">\(\lambda\)</span> an eigenvalue of <span class="math notranslate nohighlight">\(A\)</span>. Then,</p>
<p><strong>geometric multiplicity of <span class="math notranslate nohighlight">\(\lambda\)</span> <span class="math notranslate nohighlight">\(\leq\)</span> algebraic multiplicity of <span class="math notranslate nohighlight">\(\lambda\)</span></strong></p>
<p>where the <strong>algebraic multiplicity</strong> of <span class="math notranslate nohighlight">\(\lambda\)</span> is its multiplicity as a root of the characteristic polynomial of <span class="math notranslate nohighlight">\(A\)</span> and the <strong>geometric multiplicity</strong> of <span class="math notranslate nohighlight">\(\lambda\)</span> is the number of independent eigenvectors in the eigenspace of <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
</div>
<p>This means that for <a class="reference internal" href="#equation-eq-repeated-root">()</a> there could be one or two independent eigenvectors in the eigenspace of <span class="math notranslate nohighlight">\(\lambda_2\)</span>. If there are two, then the matrix is diagonalisable; if only one then it is not diagonalisable. To check, we have to calculate the null space of <span class="math notranslate nohighlight">\(A-\lambda_2I\)</span>.</p>
<!--
```{admonition} Example
:class: tip

TODO

matrix with repeated root and dimension 1 or 2 null space.
```
-->
</div>
<div class="section" id="matrix-powers-a-k">
<h2>Matrix Powers <span class="math notranslate nohighlight">\(A^k\)</span><a class="headerlink" href="#matrix-powers-a-k" title="Permalink to this headline">¶</a></h2>
<p>The eigenvector matrix <span class="math notranslate nohighlight">\(X\)</span> produces <span class="math notranslate nohighlight">\(A=X\Lambda X^{-1}\)</span>. This factorisation is useful for computing powers because <span class="math notranslate nohighlight">\(X^{-1}\)</span> multiplies with <span class="math notranslate nohighlight">\(X\)</span> to get <span class="math notranslate nohighlight">\(I\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}A^2 &amp;= X\Lambda X^{-1}X\Lambda X^{-1} = X\Lambda I\Lambda X^{-1}=X\Lambda^2X^{-1}\\
A^3 &amp; = X\Lambda^2X^{-1}X\Lambda X^{-1} = X\Lambda^3X^{-1}\end{align*}\end{split}\]</div>
<p>and so on. Because <span class="math notranslate nohighlight">\(\Lambda\)</span> is a diagonal matrix, its powers <span class="math notranslate nohighlight">\(\Lambda^k\)</span> are easy to calculate.</p>
<div class="admonition-theorem admonition">
<p class="admonition-title">Theorem</p>
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be a diagonalisable square matrix with <span class="math notranslate nohighlight">\(A = X\Lambda X^{-1}\)</span> and <span class="math notranslate nohighlight">\(k\in \mathbb{N}\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}A^k = X\Lambda^k X^{-1} = X\begin{pmatrix}\lambda_1^k&amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;\lambda_n^k \end{pmatrix}X^{-1}.\end{split}\]</div>
</div>
</div>
<div class="section" id="complex-eigenvalues">
<h2>Complex Eigenvalues<a class="headerlink" href="#complex-eigenvalues" title="Permalink to this headline">¶</a></h2>
<p>We have seen that from a square matrix <span class="math notranslate nohighlight">\(A\)</span> we can calculate the characteristic polynomial <span class="math notranslate nohighlight">\(f(\lambda)\)</span>. For an <span class="math notranslate nohighlight">\((n \times n)\)</span> matrix the polynomial is degree <span class="math notranslate nohighlight">\(n\)</span>:</p>
<div class="math notranslate nohighlight">
\[f(\lambda) = \lambda^n + a_{n-1}\lambda^{n-1} + \cdots + a_1\lambda + a_0\]</div>
<p>where <span class="math notranslate nohighlight">\(a_i\)</span> are real numbers.</p>
<p>By the fundamental theorem of algebra, <span class="math notranslate nohighlight">\(f(\lambda)\)</span> can be factorised into <span class="math notranslate nohighlight">\(n\)</span> factors <span class="math notranslate nohighlight">\(\lambda - \lambda_i\)</span> (some of which may be repeated):</p>
<div class="math notranslate nohighlight">
\[f(\lambda) = (\lambda - \lambda_1)(\lambda - \lambda_2) \cdots (\lambda - \lambda_n).\]</div>
<p>The roots <span class="math notranslate nohighlight">\(\lambda_i\)</span> are the eigenvalues of the matrix.</p>
<p>In this section we consider the case where some of the roots are not real numbers.</p>
<div class="section" id="rotations-in-2d">
<h3>Rotations in 2D<a class="headerlink" href="#rotations-in-2d" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be the matrix of an anticlockwise rotation <span class="math notranslate nohighlight">\(\pi/2\)</span> around the origin:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = \begin{pmatrix}0&amp;-1\\1&amp;0\end{pmatrix}.\end{split}\]</div>
<p>The characteristic polynomials is <span class="math notranslate nohighlight">\(\det(A-\lambda I)\)</span> which equals</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{vmatrix}-\lambda&amp;-1\\1&amp;-\lambda\end{vmatrix} = \lambda^2+1.\end{split}\]</div>
<p>The polynomial <span class="math notranslate nohighlight">\(\lambda^2+1\)</span> does not have real roots. Its roots are <span class="math notranslate nohighlight">\(\pm i\)</span> where <span class="math notranslate nohighlight">\(i\)</span> is the imaginary number <span class="math notranslate nohighlight">\(\sqrt{-1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\lambda^2+1 = (\lambda+i)(\lambda-i)\]</div>
<p>resulting in two complex eigenvalues <span class="math notranslate nohighlight">\(\lambda_1 = i\)</span> and <span class="math notranslate nohighlight">\(\lambda_2= -i\)</span>.</p>
<p>We also find that the eigenvectors contain the imaginary number <span class="math notranslate nohighlight">\(i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A - \lambda_1I = \begin{pmatrix}-i&amp;-1\\1&amp;-i\end{pmatrix} \underrightarrow{\mathrm{~RREF~}} \begin{pmatrix}1&amp;-i\\0&amp;0\end{pmatrix}\end{split}\]</div>
<p>and hence the eigenvector corresponding to the eigenvalue <span class="math notranslate nohighlight">\(\lambda_1 = i\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}v_1 = \begin{pmatrix}i\\1\end{pmatrix}.\end{split}\]</div>
<p>Likewise the eigenspace corresponding to the eigenvalue <span class="math notranslate nohighlight">\(\lambda_2 = -i\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}v_2 = \begin{pmatrix}-i\\1\end{pmatrix}.\end{split}\]</div>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>Find the eigenvalues of an anticlockwise rotation by an angle <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p><strong>Solution</strong></p>
<p>The matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = \begin{pmatrix}\cos\theta&amp;-\sin\theta\\\sin\theta&amp;\cos\theta\end{pmatrix}\end{split}\]</div>
<p>represents an anticlockwise rotation by an angle <span class="math notranslate nohighlight">\(\theta\)</span>. The characteristic polynomial <span class="math notranslate nohighlight">\(f(\lambda)\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}\det(A-\lambda I) &amp;= \begin{vmatrix}\cos\theta-\lambda&amp;-\sin\theta\\\sin\theta&amp;\cos\theta-\lambda\end{vmatrix}\\
&amp;= (\cos\theta - \lambda)^2+\sin^2\theta.\end{align*}\end{split}\]</div>
<p>Setting this to zero and solving for <span class="math notranslate nohighlight">\(\lambda\)</span> gives eigenvalues</p>
<div class="math notranslate nohighlight">
\[\lambda = \cos\theta\pm i\sin\theta = e^{\pm i\theta}.\]</div>
</div>
</div>
</div>
<div class="section" id="trace-and-determinant">
<h2>Trace and Determinant<a class="headerlink" href="#trace-and-determinant" title="Permalink to this headline">¶</a></h2>
<p>Calculating eigenvalues is (in general) a difficult problem. However, in some cases we can use some ‘tricks’ to help find them.</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>The <strong>trace</strong> of a matrix is the sum of the diagonal entries. Given a matrix <span class="math notranslate nohighlight">\(A\)</span> with entries <span class="math notranslate nohighlight">\(a_{ij}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathrm{tr}(A) = a_{11} + \cdots +a_{nn}.\]</div>
</div>
<div class="admonition-theorem admonition">
<p class="admonition-title">Theorem</p>
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be an <span class="math notranslate nohighlight">\((n \times n)\)</span> matrix with eigenvalues <span class="math notranslate nohighlight">\(\lambda_1, \ldots, \lambda_n\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\lambda_1 + \cdots + \lambda_n = \mathrm{tr}(A)\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\lambda_1\lambda_2 \cdots \lambda_n = \det(A).\]</div>
<p>The sum of the eigenvalues is the sum of the diagonal entries of <span class="math notranslate nohighlight">\(A\)</span>. The product of the eigenvalues is the determinant of <span class="math notranslate nohighlight">\(A\)</span>.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<p>Calculate the determinant of the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = \begin{pmatrix}1&amp;1&amp;1\\1&amp;1&amp;1\\1&amp;1&amp;1\end{pmatrix}.\end{split}\]</div>
<p><strong>Solution</strong></p>
<p>This matrix is clearly not invertible, and so has zero determinant and at least one zero eigenvalue.</p>
<p>In fact there are two independent eigenvectors in the zero eigenspace (check this!). This means that we have <span class="math notranslate nohighlight">\(\lambda_1=\lambda_2=0\)</span>. To find the third eigenvalue, use the identity</p>
<div class="math notranslate nohighlight">
\[\lambda_1 + \lambda_2 + \lambda_3 = \mathrm{tr}(A)\]</div>
<p>to determine that <span class="math notranslate nohighlight">\(\lambda_3=3\)</span>.</p>
</div>
</div>
<div class="section" id="solutions-to-exercises">
<h2>Solutions to Exercises<a class="headerlink" href="#solutions-to-exercises" title="Permalink to this headline">¶</a></h2>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./LinearAlgebra\linear_systems_matrices"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Natural Sciences<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>